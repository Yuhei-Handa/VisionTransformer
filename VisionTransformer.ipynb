{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torchvision\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "transform =torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(image_size), torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_datasets = torchvision.datasets.CIFAR100(\n",
    "    root=\"./data\", train=True, transform=transform, download=True\n",
    ")\n",
    "\n",
    "test_datasets = torchvision.datasets.CIFAR100(\n",
    "    root=\"./data\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_datasets, batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_datasets, batch_size=32, shuffle=False\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = next(iter(train_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9020, 0.9020, 0.9020,  ..., 0.9020, 0.9020, 0.9020],\n",
       "        [0.9020, 0.9020, 0.9020,  ..., 0.9020, 0.9020, 0.9020],\n",
       "        [0.9020, 0.9020, 0.9020,  ..., 0.9020, 0.9020, 0.9020],\n",
       "        ...,\n",
       "        [0.3922, 0.3922, 0.3922,  ..., 0.4588, 0.4588, 0.4588],\n",
       "        [0.3922, 0.3922, 0.3922,  ..., 0.4588, 0.4588, 0.4588],\n",
       "        [0.3922, 0.3922, 0.3922,  ..., 0.4588, 0.4588, 0.4588]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "patch_size = 2\n",
    "num_channel = 3\n",
    "batch_size = 32\n",
    "patch_window = torch.ones((patch_size, patch_size), dtype=torch.long)\n",
    "patch_window = patch_window.expand(num_channel, patch_size, patch_size).unsqueeze(0).expand(batch_size, num_channel, patch_size, patch_size)\n",
    "\n",
    "\n",
    "patch = image[:, :, :2, :2] * patch_window\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 5])\n"
     ]
    }
   ],
   "source": [
    "position_ids = torch.tensor(list(range(10)), dtype=torch.long).expand(4, -1)\n",
    "\n",
    "embed = nn.Embedding(10, 5)\n",
    "\n",
    "print(embed(position_ids).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "cls_embedding = nn.Embedding(1, 768)(torch.tensor(0))\n",
    "print(cls_embedding.unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=5, bias=True)\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "\n",
    "for _ in range(8):\n",
    "    l.append(nn.Linear(5, 5))\n",
    "\n",
    "l = nn.ModuleList(l)\n",
    "print(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_hidden_size, num_layer, num_head = 758, 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm((32, 12545, 758), eps=1e-05, elementwise_affine=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class VisonTransformer(nn.Module):\n",
    "    def __init__(self, batch_size, image_size, num_channel, patch_size, embed_hidden_size, num_layer, num_head):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "        self.num_channel = num_channel\n",
    "        self.num_patch = int((image_size / patch_size) * (image_size / patch_size))\n",
    "        self.num_token = self.num_patch + 1\n",
    "        self.num_layer = num_layer\n",
    "        self.num_head = num_head        \n",
    "        self.cls_id = torch.tensor(0, dtype=torch.long)\n",
    "        self.cls_embedding = nn.Embedding(1, embed_hidden_size)\n",
    "        self.embed_hidden_size = embed_hidden_size\n",
    "        self.positional_embedding = nn.Embedding(self.num_token, embed_hidden_size)\n",
    "        self.image_embedding = nn.Linear(patch_size * patch_size * self.num_patch, embed_hidden_size)\n",
    "        self.encode_layer = encoder()\n",
    "\n",
    "\n",
    "    def image_to_token(self, images, patch_size):\n",
    "        batch_size, num_channel, width, height = self.batch_size, self.num_channel, self.image_size, self.image_size\n",
    "        num_patch = self.num_patch\n",
    "        num_pixel_in_patch = patch_size * patch_size\n",
    "\n",
    "        patch_window = torch.ones((patch_size, patch_size), dtype=torch.long)\n",
    "        patch_window = patch_window.unsqueeze(0).expand(num_channel, patch_size, patch_size)\\\n",
    "            .unsqueeze(0).expand(batch_size, num_channel, patch_size, patch_size)\n",
    "        \n",
    "        token_list = torch.tensor([])\n",
    "\n",
    "        for row_idx, in range(0, width, patch_size):\n",
    "            for col_idx in range(0, height, patch_size):\n",
    "                token_list = torch.concat([token_list, \\\n",
    "                                           images[row_idx:row_idx + patch_size, col_idx:col_idx + patch_size]], dim=0)\n",
    "                \n",
    "        \n",
    "        token_list = token_list.transpose(1, 0, 2, 3, 4).view(batch_size, num_patch, num_channel, -1)\n",
    "        token_list = token_list.view(batch_size, num_patch, -1)\n",
    "\n",
    "        return token_list\n",
    "    \n",
    "    def positional_encoding(self, num_token):\n",
    "        position_ids = torch.tensor(list(range(num_token)), dtype=torch.long).expand(self.batch_size, -1)\n",
    "        positional_embeds = self.positional_embedding(position_ids)\n",
    "\n",
    "        return positional_embeds\n",
    "    \n",
    "    def setup_encoder(self, num_layer):\n",
    "        encoder_list = []\n",
    "        for _ in range(num_layer):\n",
    "            encoder_list.append(self.encode_layer)\n",
    "\n",
    "        encoder_layer = nn.ModuleList()\n",
    "\n",
    "    \n",
    "    def forward(self, images):\n",
    "        \n",
    "        cls_tokens = self.cls_embedding(self.cls_id).unsqueeze(0).expand(batch_size, 1, -1)\n",
    "        image_tokens = self.image_to_token(images, self.patch_size)\n",
    "        tokens = torch.concat([cls_tokens, image_tokens], dim=1)\n",
    "        positional_embeds = self.positional_encoding(self.num_token)\n",
    "\n",
    "        return positional_embeds\n",
    "\n",
    "\n",
    "class encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(batch_size, image_size, num_channel, patch_size, embed_hidden_size, num_layer, num_head)\n",
    "        num_patch = int((image_size / patch_size) * (image_size / patch_size))\n",
    "        num_token = num_patch + 1\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm((batch_size, num_token, embed_hidden_size))\n",
    "        self.layer_norm2 = nn.LayerNorm((batch_size, num_token, embed_hidden_size))\n",
    "        self.mlp = nn.Linear(embed_hidden_size, embed_hidden_size)\n",
    "        self.attention_layer = MultiHeadAttention()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        layer_norm1 = self.layer_norm1(tokens)\n",
    "        skip1 = tokens\n",
    "        concat_attention = self.attention_layer(layer_norm1)\n",
    "        outputs_tmp1 = concat_attention + skip1\n",
    "        skip2 = outputs_tmp1\n",
    "        layer_norm2 = self.layer_norm2(outputs_tmp1)\n",
    "        mlp = self.mlp(layer_norm2)\n",
    "        outputs = mlp + skip2\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_hidden_size, num_head):\n",
    "        super().__init__()\n",
    "        self.attention_layers = []\n",
    "        self.query_layers = []\n",
    "        self.key_layers = []\n",
    "        self.value_layers = []       \n",
    "\n",
    "    def output_attention(self, tokens, num_head):\n",
    "\n",
    "        multi_embed_hidden_size = int(embed_hidden_size / num_head)\n",
    "\n",
    "        for number in range(num_head):\n",
    "            self.query_layers.append(nn.Linear(embed_hidden_size, multi_embed_hidden_size))\n",
    "            self.key_layers.append(nn.Linear(embed_hidden_size, multi_embed_hidden_size))\n",
    "            self.value_layers.append(nn.Linear(embed_hidden_size, multi_embed_hidden_size))\n",
    "        self.query_layers = nn.ModuleList(self.query_layers)\n",
    "        self.key_layers = nn.ModuleList(self.key_layers)\n",
    "        self.value_layers = nn.ModuleList(self.value_layers)\n",
    "\n",
    "\n",
    "        for number in range(num_head):\n",
    "            query = self.query_layers[number](tokens)\n",
    "            key = self.key_layers[number](tokens)\n",
    "            value = self.value_layers[number](tokens)\n",
    "\n",
    "            attention = nn.Softmax((query@torch.transpose(key, 1, 2)) / torch.sqrt(multi_embed_hidden_size), dim=-1)@value\n",
    "\n",
    "            if number > 0:concat_attention = torch.concat([concat_attention, attention], dim=-1)\n",
    "            else:concat_attention = attention\n",
    "\n",
    "        return concat_attention\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "\n",
    "        concat_attention = self.output_attention(tokens, self.num_head)\n",
    "\n",
    "        return concat_attention\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
